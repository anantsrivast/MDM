{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep:1 \\n\\nn=number of min hashes\\nb=number of bands\\nr=number of min hash functions for each band\\n1. Create a hashmap(dict) of (26+10)*(26+10) possible values(00...zz) and broadcast.\\n2. Generate n=b*r hash functions and broadcast. To make these functions. create b*r arrays of 1296 elements(-1 and 1)\\n2.1 For each document, create shingles(bi grams).\\n2.2 Perform following step n times for each of the hash functions\\n    Apply hash fuction to each ngram(or a subset of shingles-random selection for a large doc) and \\n    store the minhash for that iteration\\n1.3 Now you have a list of minhashes for a document. Split it into b bands of r minhashes each\\n    and apply a hash function/concat to generate a min hash for the array of r minhashes\\n    Now emit(hash(list(minhash)),band id),doc id\\n1.4 group all doc ids by hash(list(minhash)\\n1.5 these are the matching documents.\\n1.6 Thrshold t~ (1/b) pow (1/r). So tune accordingly\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step:1 \n",
    "\n",
    "n=number of min hashes\n",
    "b=number of bands\n",
    "r=number of min hash functions for each band\n",
    "1. Create a hashmap(dict) of (26+10)*(26+10) possible values(00...zz) and broadcast.\n",
    "2. Generate n=b*r hash functions and broadcast. To make these functions. create b*r arrays of 1296 elements(-1 and 1)\n",
    "2.1 For each document, create shingles(bi grams).\n",
    "2.2 Perform following step n times for each of the hash functions\n",
    "    Apply hash fuction to each ngram(or a subset of shingles-random selection for a large doc) and \n",
    "    store the minhash for that iteration\n",
    "1.3 Now you have a list of minhashes for a document. Split it into b bands of r minhashes each\n",
    "    and apply a hash function/concat to generate a min hash for the array of r minhashes\n",
    "    Now emit(hash(list(minhash)),band id),doc id\n",
    "1.4 group all doc ids by hash(list(minhash)\n",
    "1.5 these are the matching documents.\n",
    "1.6 Thrshold t~ (1/b) pow (1/r). So tune accordingly\n",
    "\n",
    "'''\n",
    "\n",
    "##Read the file into a RDD and then create ngrams/shingles hash each string to integer \n",
    "##using CRC32\n",
    "##Do not need random selection of shingles since the word list is not quite large\n",
    "##In the mapper, apply hashCRC to each shingle and then  apply n hash functions and create a list of minhashes.\n",
    "##Now in the same  mapper, Prepare n/x =1000/50 =20 bands and emit (band id,hash of sub-list of min hashes),DOC ID\n",
    "## In combiner group all doc ids by hash codes and band id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def hashCRC(ngram):\\n    import binascii \\n    crc = binascii.crc32(ngram) & 0xffffffff\\n    return crc\\n\\ndef ngramsMinHash(line,tuple_size,num_buckets,num_bands,h1,max_prime,coeff):\\n  \\n  words=line.split(\",\")\\n  arr=[]\\n \\n  import re\\n  name=re.sub(r\\'\\\\s+\\', \\'\\', words[1]+words[2]).lower()\\n  \\n  for i in xrange((len(name)-tuple_size+1)):\\n    #print (name[i]+name[i+1],words[0])\\n    j=1\\n    tempGram=\"\"\\n    while(j<tuple_size):\\n        tempGram=tempGram+name[i+j]\\n        j+=1\\n        \\n        \\n    arr.append(hashCRC(name[i]+tempGram))\\n  \\n  num_hashes=num_buckets/num_bands\\n  minHash=[[] for i in xrange(num_bands)]\\n \\n  for i in xrange(num_buckets):\\n        hash_each_iter=[]\\n       \\n        for val in arr:\\n            hash_each_iter.append(h1(val,max_prime,coeff[0][i],coeff[1][i]))\\n        \\n        #print min(hash_each_iter)\\n        minHash[int(i/num_hashes)].append(str(min(hash_each_iter)))\\n       \\n    \\n  ##Now apply another hash function to each stripe of each band and emit  and hash (value, bandid), doc id  \\n  ## This will be reduced and similar pairs emitted\\n \\n  print name,words[0]\\n  for i in xrange(len(minHash)):\\n     #yield (i,hash(tuple(minHash[i]))),words[0]\\n     yield (i,\\'\\'.join(minHash[i])),words[0]   \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def hashCRC(ngram):\n",
    "    import binascii \n",
    "    crc = binascii.crc32(ngram) & 0xffffffff\n",
    "    return crc\n",
    "\n",
    "def ngramsMinHash(line,tuple_size,num_buckets,num_bands,h1,max_prime,coeff):\n",
    "  \n",
    "  words=line.split(\",\")\n",
    "  arr=[]\n",
    " \n",
    "  import re\n",
    "  name=re.sub(r'\\s+', '', words[1]+words[2]).lower()\n",
    "  \n",
    "  for i in xrange((len(name)-tuple_size+1)):\n",
    "    #print (name[i]+name[i+1],words[0])\n",
    "    j=1\n",
    "    tempGram=\"\"\n",
    "    while(j<tuple_size):\n",
    "        tempGram=tempGram+name[i+j]\n",
    "        j+=1\n",
    "        \n",
    "        \n",
    "    arr.append(hashCRC(name[i]+tempGram))\n",
    "  \n",
    "  num_hashes=num_buckets/num_bands\n",
    "  minHash=[[] for i in xrange(num_bands)]\n",
    " \n",
    "  for i in xrange(num_buckets):\n",
    "        hash_each_iter=[]\n",
    "       \n",
    "        for val in arr:\n",
    "            hash_each_iter.append(h1(val,max_prime,coeff[0][i],coeff[1][i]))\n",
    "        \n",
    "        #print min(hash_each_iter)\n",
    "        minHash[int(i/num_hashes)].append(str(min(hash_each_iter)))\n",
    "       \n",
    "    \n",
    "  ##Now apply another hash function to each stripe of each band and emit  and hash (value, bandid), doc id  \n",
    "  ## This will be reduced and similar pairs emitted\n",
    " \n",
    "  print name,words[0]\n",
    "  for i in xrange(len(minHash)):\n",
    "     #yield (i,hash(tuple(minHash[i]))),words[0]\n",
    "     yield (i,''.join(minHash[i])),words[0]   \n",
    "'''\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import binascii \n",
    "import re\n",
    "from collections import Counter\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def bigram_map():\n",
    "    \n",
    "    a=['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    c=0\n",
    "    d={}\n",
    "    for i in a:\n",
    "        for j in a:\n",
    "           d[i+j]=c\n",
    "           c+=1\n",
    "    return d\n",
    "\n",
    "def ngrams_min_hash(line,tuple_size,coeffs,num_bands):\n",
    "  \n",
    "  words=line.split(\",\",1)\n",
    "  arr=[]\n",
    "  dict_bigram=bigram_map()\n",
    "  \n",
    "  name_temp=re.sub(r'\\s+', '', words[1])\n",
    "  name=re.sub(\"[^\\w]\",'',name_temp).lower()\n",
    " \n",
    "  for i in xrange((len(name)-tuple_size+1)):\n",
    "    j=1\n",
    "    tempGram=\"\"\n",
    "    while(j<tuple_size):\n",
    "        tempGram=tempGram+name[i+j]\n",
    "        j+=1\n",
    "        \n",
    "        \n",
    "    arr.append(dict_bigram[name[i]+tempGram])\n",
    "  \n",
    "  sv=SparseVector(1296,dict(Counter(arr)))\n",
    "  arr=[]\n",
    "  for i in xrange(num_bands):\n",
    "        yield words[0],i,sv  \n",
    "            \n",
    "           \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\ndef gen_hash_coeffs(max_prime):\\n    \\n    a1=np.random.randint(0, max_prime, size=1)[0]\\n    b1=np.random.randint(0, max_prime, size=1)[0]\\n    a2=np.random.randint(0, max_prime, size=1)[0]\\n    b2=np.random.randint(0, max_prime, size=1)[0]\\n    #return[a1,b1,a2,b2]\\n    \\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "def gen_hash_coeffs(max_prime):\n",
    "    \n",
    "    a1=np.random.randint(0, max_prime, size=1)[0]\n",
    "    b1=np.random.randint(0, max_prime, size=1)[0]\n",
    "    a2=np.random.randint(0, max_prime, size=1)[0]\n",
    "    b2=np.random.randint(0, max_prime, size=1)[0]\n",
    "    #return[a1,b1,a2,b2]\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "def gen_hash_coeffs_1(num_buckets,num_bigrams,num_bands,seed):\n",
    "    #coeff=[[] for j in xrange(num_bands)]\n",
    "    coeff=[None]*num_bands\n",
    "    r=num_buckets/num_bands\n",
    "    #arr=[[] for j in xrange(r)]\n",
    "    arr=[None]*r\n",
    "    for i in xrange(num_buckets):\n",
    "        nRDD= RandomRDDs.normalRDD(sc, num_bigrams, seed=seed+i).map(lambda val: str(-1) if val < 0 else str(1)).collect()\n",
    "        j=nRDD\n",
    "        idx= i%(r)\n",
    "     \n",
    "        arr[idx]=(idx+1,j)\n",
    "        if i%r ==r-1:\n",
    "            coeff[i/r]=arr\n",
    "            arr=[None]*r\n",
    "    return coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coeff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4913b70f31b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'coeff' is not defined"
     ]
    }
   ],
   "source": [
    "len(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH(line,coeff):\n",
    "    r=(coeff.value)[line[1]]\n",
    "    hash_str=''\n",
    "    for i in xrange(len(r)):\n",
    "        hash_str=hash_str+str(1 if line[2].dot(r[i][1]).sum() > 0 else 0)\n",
    "    return hash_str,line[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 9:58AM 52 seconds CDT on Apr 10, 2017'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "#time.ctime() # 'Mon Oct 18 13:35:29 2010'\n",
    "time.strftime('%l:%M%p %S seconds %Z on %b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#worked for small list NamesLSH\n",
    "#num_buckets=1000\n",
    "#num_bands=50\n",
    "##worked better for NamesLSH\n",
    "#num_bands=15 \n",
    "#num_buckets=210\n",
    "num_bands=20 \n",
    "num_buckets=400\n",
    "coeffs=sc.broadcast(gen_hash_coeffs_1(num_buckets,1296,num_bands,100))\n",
    "#coeffs\n",
    "#coeffs\n",
    "#print coeffs.collect()\n",
    "#print coeffs\n",
    "##Create a dictionary of bigrams:\n",
    "#print coeffs.map(lambda sv:).collect()\n",
    "\n",
    "#rdd_name=sc.textFile(\"/Users/anant/Documents/Projects/SingleView/us-names-1.txt\")\n",
    "rdd_name=sc.textFile(\"/Users/anant/Documents/Projects/SingleView/100.txt\")\n",
    "#rdd_name=sc.textFile(\"/Users/anant/Documents/Projects/SingleView/NamesLSH.txt\")\n",
    "#sc.broadCast(coeffs)\n",
    "#match_buckets=rdd_name.flatMap(lambda line: ngrams_min_hash(line,2,num_buckets,num_bands,coeffs)).collect()\n",
    "#match_buckets=rdd_name.flatMap(lambda line: ngrams_min_hash(line,2,num_buckets,num_bands,coeffs)).combineByKey(lambda value:[value],lambda x,value:x+[value],lambda x,value:x+value).filter(lambda (x,y): len(y)>= 2).collect()\n",
    "doc_vector=rdd_name.flatMap(lambda line: ngrams_min_hash(line,2,coeffs,num_bands)).map(lambda line:LSH(line,coeffs)).combineByKey(lambda value:[value],lambda x,value:x+[value],lambda x,value:x+value).filter(lambda (x,y): len(y)>= 2).map(lambda val: (','.join(val[1]),val[0])).reduceByKey(lambda x,y: x)\n",
    "#matches=doc_vector.map(lambda line:coeffs.map(lambda coeff: ((line[0],coeff[0]/num_bands),(coeff[0]%num_bands,(1 if line[1].dot(coeff[1]).sum() > 0 else 0))))).collect()\n",
    "#biGrams=rdd_name.flatMap(lambda line: ngramsMinHash(line,2,50,25,h1,h2,max_prime)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'16,17', '10110000010101110010'), (u'1,2', '00101010101001110011'), (u'18,19', '10100001010101110101')]\n"
     ]
    }
   ],
   "source": [
    "x=sqlContext.createDataFrame(doc_vector, [\"matches\", \"mKey\"])\n",
    "x.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://localhost:27017/\").option(\"database\",\"spark\").option(\"collection\",\"test\").mode(\"overwrite\").save()\n",
    "matches= doc_vector.collect()\n",
    "print matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1:28PM 19 seconds CDT on Apr 10, 2017'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "#time.ctime() # 'Mon Oct 18 13:35:29 2010'\n",
    "time.strftime('%l:%M%p %S seconds %Z on %b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match-1\n",
      "aaron, bill r\n",
      "aaron, billy\n",
      "\n",
      "\n",
      "\n",
      "Match-2\n",
      "abbott, erik t\n",
      "abbott, eric\n",
      "\n",
      "\n",
      "\n",
      "Match-3\n",
      "abbott, erik t\n",
      "abbott, j\n",
      "\n",
      "\n",
      "\n",
      "Match-4\n",
      "aaron, billy\n",
      "aaron, b\n",
      "\n",
      "\n",
      "\n",
      "Match-5\n",
      "abbott, jerry l\n",
      "abbott, jerry w\n",
      "\n",
      "\n",
      "\n",
      "Match-6\n",
      "aaronson, cary\n",
      "abbate, gary w\n",
      "\n",
      "\n",
      "\n",
      "Match-7\n",
      "aaron, bill r\n",
      "aaron, b\n",
      "\n",
      "\n",
      "\n",
      "Match-8\n",
      "abbott, eric\n",
      "abbott, jerry w\n",
      "\n",
      "\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "my_set=set()\n",
    "#with open('/Users/anant/Documents/Projects/SingleView/us-names-1.txt') as f:\n",
    "with open('/Users/anant/Documents/Projects/SingleView/100.txt') as f:\n",
    "    for line in f:\n",
    "        #print line\n",
    "        vals=line.split(\",\",1)\n",
    "        #print vals[1].strip()\n",
    "        d[vals[0].strip()]=vals[1].strip()\n",
    "\n",
    "for i in xrange(len(matches)):\n",
    "    \n",
    "    print \"Match-\"+str(i+1)\n",
    "    names=matches[i][0].split(\",\")\n",
    "    \n",
    "    for j in xrange(len(names)):\n",
    "        my_set.add(d[str(names[j])])\n",
    "        print d[names[j]]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "print len(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x102e85c50>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.saveToMongoDB('mongodb://localhost:27017/spark.test.output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
